{"cells":[{"cell_type":"markdown","metadata":{"id":"qgtmB5jdEb4G"},"source":["# KAGGLE: Feedback Prize - English Language Learning\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T18:35:13.795172Z","iopub.status.busy":"2022-12-01T18:35:13.794473Z","iopub.status.idle":"2022-12-01T18:35:13.839281Z","shell.execute_reply":"2022-12-01T18:35:13.838292Z","shell.execute_reply.started":"2022-12-01T18:35:13.795023Z"},"id":"XO01TJHeEjSp","tags":[],"trusted":true},"outputs":[],"source":["NAME_PROJECT = \"Feedback Prize - English Language Learning\"\n","import time\n","\n","# Config\n","class CFG:\n","    model_name = \"microsoft/deberta-v3-large\"\n","\n","    # hyperparameters\n","    transformer_lr     = 1e-6\n","    head_lr            = 3e-4\n","    batch_size         = 2\n","    \n","    accumulation_steps = 1\n","    warmup_epochs      = 0.33333333\n","    max_epochs         = 5\n","    adam_eps           = 1e-5\n","    scheduler          = \"linear\"\n","    max_grad_norm      = 1\n","    head_dropout       = 0.2\n","    rnn_dropout        = 0.\n","\n","    # data split\n","    num_folds          = 5\n","    train_on_full_data = False\n","    current_fold       = 0\n","\n","    # others\n","    # seed = int(time.time())\n","    seed                = 0\n","    eval_batch_size     = 8\n","    debug               = False\n","    fp16                = True\n","    gradient_checkpoint = False\n","    optimizer_bit8      = False\n","    use_augmentation    = False\n","    new_line_token      = True\n","\n","    # AWP\n","    use_awp         = True\n","    adversarial_lr  = 1e-5\n","    adversarial_eps = 1e-3\n","    start_awp_epoch = 1\n","    awp_steps       = 1\n","\n","\n","IDS_TO_LABELS = {\n","    0 : \"cohesion\",\n","    1 : \"syntax\",\n","    2 : \"vocabulary\",\n","    3 : \"phraseology\",\n","    4 : \"grammar\",\n","    5 : \"conventions\"\n","}"]},{"cell_type":"markdown","metadata":{"id":"hmD6SOYTAduZ"},"source":["### 1) Imports\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T18:35:13.844512Z","iopub.status.busy":"2022-12-01T18:35:13.844218Z","iopub.status.idle":"2022-12-01T18:35:43.207297Z","shell.execute_reply":"2022-12-01T18:35:43.205996Z","shell.execute_reply.started":"2022-12-01T18:35:13.844485Z"},"id":"o6yecuGHArMe","trusted":true},"outputs":[],"source":["%%capture\n","import os\n","\n","IS_KAGGLE = bool(os.environ.get('KAGGLE_KERNEL_RUN_TYPE', ''))\n","IS_COLAB = bool(os.environ.get(\"COLAB_GPU\"))\n","IS_PAPER = bool(os.environ.get(\"PAPERSPACE_NOTEBOOK_ID\"))\n","\n","if IS_COLAB:\n","    !pip install wandb\n","    !pip install transformers\n","    !pip install sentencepiece\n","    !pip install pytorch_lightning\n","    !pip install iterative-stratification\n","\n","if IS_KAGGLE:\n","    !pip install --upgrade transformers\n","    !pip install iterative-stratification\n","\n","if IS_PAPER:\n","    !pip install pytorch_lightning\n","    !pip install wandb\n","    !pip install kaggle\n","    !pip install iterative-stratification\n","\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T18:35:43.211106Z","iopub.status.busy":"2022-12-01T18:35:43.210027Z","iopub.status.idle":"2022-12-01T18:35:53.308368Z","shell.execute_reply":"2022-12-01T18:35:53.307308Z","shell.execute_reply.started":"2022-12-01T18:35:43.211062Z"},"id":"5MXqXYoEAj1Y","outputId":"807d1e59-798d-401f-d866-f136e11ebbc1","trusted":true},"outputs":[],"source":["import random\n","import pandas as pd\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.checkpoint import checkpoint\n","\n","import pytorch_lightning as pl\n","from sklearn.metrics import mean_squared_error\n","\n","import transformers\n","\n","import wandb\n","\n","import warnings, logging\n","warnings.simplefilter('ignore')\n","logging.disable(logging.WARNING)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T18:35:53.312475Z","iopub.status.busy":"2022-12-01T18:35:53.311693Z","iopub.status.idle":"2022-12-01T18:35:53.32022Z","shell.execute_reply":"2022-12-01T18:35:53.318543Z","shell.execute_reply.started":"2022-12-01T18:35:53.312434Z"},"id":"Tr94OJfEzcEu","outputId":"c5c1ee5c-fe4a-43e6-eb4c-c1a85b2f518e","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1.11.0+cu113\n"]}],"source":["print(torch.__version__)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T18:35:53.322284Z","iopub.status.busy":"2022-12-01T18:35:53.321777Z","iopub.status.idle":"2022-12-01T18:35:53.399901Z","shell.execute_reply":"2022-12-01T18:35:53.398584Z","shell.execute_reply.started":"2022-12-01T18:35:53.322248Z"},"id":"PhzhxNTLBhxt","trusted":true},"outputs":[],"source":["CFG.num_jobs = os.cpu_count()\n","CFG.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","CFG.device_name = \"CPU\"\n","\n","if CFG.device == \"cuda\":\n","    CFG.device_name = torch.cuda.get_device_name(0)\n","    \n","    if CFG.optimizer_bit8:\n","        !pip install bitsandbytes\n","        import bitsandbytes as bnb\n","\n","DBS_FROM_KAGGLE = [\n","    (\"feedback-prize-english-language-learning\", True),\n","    (\"feedback-prize-effectiveness\", True),\n","    (\"feedback-prize-2021\", True)\n","]"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T18:35:53.401898Z","iopub.status.busy":"2022-12-01T18:35:53.401504Z","iopub.status.idle":"2022-12-01T18:35:53.411752Z","shell.execute_reply":"2022-12-01T18:35:53.410396Z","shell.execute_reply.started":"2022-12-01T18:35:53.401861Z"},"id":"Ul3BUIB1Dsfb","trusted":true},"outputs":[],"source":["# SECRETS \n","os.environ[\"KAGGLE_USERNAME\"] = \"\"\n","os.environ[\"KAGGLE_KEY\"] = \"\"\n","WANDB_SECRET = \"\""]},{"cell_type":"markdown","metadata":{"id":"XWGe3TQoEnpD"},"source":["### 2) General Utilities"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T18:35:53.415515Z","iopub.status.busy":"2022-12-01T18:35:53.415316Z","iopub.status.idle":"2022-12-01T18:35:53.426932Z","shell.execute_reply":"2022-12-01T18:35:53.425974Z","shell.execute_reply.started":"2022-12-01T18:35:53.415484Z"},"id":"40F9am0i0eUA","trusted":true},"outputs":[],"source":["# Utilities\n","def load_data_from_kaggle(list_of_dbs):\n","    def load_kaggle_db(full_name, is_competition):\n","        import kaggle, zipfile\n","        \n","        name = full_name\n","        if not is_competition:\n","            name = name[name.find(\"/\") + 1:]\n","        \n","        if not os.path.isdir(f\"../input/{name}\"):\n","            if is_competition:\n","                kaggle.api.competition_download_files(name)\n","                zipfile.ZipFile(f\"{name}.zip\").extractall(f\"../input/{name}\")\n","            else:\n","                kaggle.api.dataset_download_files(full_name, f\"../input/{name}\", unzip=True)\n","    \n","    for db in list_of_dbs:\n","        load_kaggle_db(*db)\n","\n","def set_random_seed(seed):\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","# if not IS_KAGGLE and DBS_FROM_KAGGLE is not None:\n","#     load_data_from_kaggle(DBS_FROM_KAGGLE)\n","set_random_seed(CFG.seed)"]},{"cell_type":"markdown","metadata":{"id":"8jJWrAunKytj"},"source":["### 3) Comp utilities"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T18:35:53.428785Z","iopub.status.busy":"2022-12-01T18:35:53.42849Z","iopub.status.idle":"2022-12-01T18:35:53.46372Z","shell.execute_reply":"2022-12-01T18:35:53.462725Z","shell.execute_reply.started":"2022-12-01T18:35:53.428745Z"},"id":"ZO6yLX6IUdQL","trusted":true},"outputs":[],"source":["### AWP ###\n","\n","class AWP:\n","    def __init__(self, model, optimizer, scaler, adv_lr, adv_eps, awp_steps):\n","        self.model, self.optimizer, self.scaler, self.adv_lr, self.adv_eps, self.awp_steps = model, optimizer, scaler, adv_lr, adv_eps, awp_steps\n","        self.backup, self.backup_eps = {}, {}\n","        \n","        self.is_f16 = CFG.fp16\n","        print(f\"AWP is training with f16 data: {self.is_f16}\")\n","        \n","    def attack_step(self):\n","        eps = 1e-6\n","        for n, p in self.model.named_parameters():\n","            if p.grad is not None and \"weight\" in n:\n","                \n","                norm1 = torch.norm(p.grad)\n","                norm2 = torch.norm(p.data.detach())\n","                \n","                if norm1 != 0 and not torch.isnan(norm1):\n","                    \n","                    r_at = self.adv_lr * p.grad / (norm1 + eps) * (norm2 + eps)\n","                    \n","                    p.data.add_(r_at)\n","                    p.data = torch.min(\n","                        torch.max(p.data, self.backup_eps[n][0]), self.backup_eps[n][1]\n","                    )\n","    \n","    def attack(self, batch):\n","        self.save()\n","        for i in range(self.awp_steps):\n","            self.attack_step()\n","            \n","            with torch.cuda.amp.autocast(enabled=self.is_f16):\n","                adv_outputs = self.model(**batch)\n","            \n","            adv_loss = adv_outputs[\"loss\"]\n","\n","            self.optimizer.zero_grad()\n","            self.scaler.scale(adv_loss).backward()\n","\n","        self.restore()\n","    \n","    def save(self):\n","        for n, p in self.model.named_parameters():\n","            if p.grad is not None and \"weight\" in n:\n","                if n not in self.backup:\n","                    data               = p.data.clone()#.cpu()\n","                    self.backup[n]     = data\n","                    grad_eps           = self.adv_eps * p.abs().detach()\n","                    self.backup_eps[n] = (data - grad_eps, data + grad_eps)\n","    \n","    def restore(self):\n","        for n, p in self.model.named_parameters():\n","            if n in self.backup:\n","                p.data = self.backup[n]#.cuda()\n","        \n","        self.backup = {}\n","        self.backup_eps = {}\n","\n","\n","### Trainer ###\n","\n","class AWPModule(pl.LightningModule):\n","    def __init__(self, model):\n","        super().__init__()\n","        self.model       = model\n","        self.best_metric = float(\"inf\")\n","        \n","        self.start_awp_epoch = CFG.start_awp_epoch\n","    \n","    def on_train_start(self):\n","        if CFG.use_awp:\n","            print(\"START AWP\")\n","            \n","            scaler = self.trainer.scaler\n","            if scaler is None:\n","                scaler = torch.cuda.amp.GradScaler(enabled=False)\n","                \n","            self.awp = AWP(model, self.optimizers().optimizer, scaler, CFG.adversarial_lr, CFG.adversarial_eps, CFG.awp_steps)\n","        else:\n","            print(\"NO AWP\")\n","            self.start_awp_epoch = 1000\n","\n","    def on_after_backward(self):\n","        if self.current_epoch >= self.start_awp_epoch:\n","            self.awp.attack(self.batch)\n","    \n","    def training_step(self, batch, idx):\n","        self.batch = batch\n","        \n","        outputs = self.model(**batch)\n","        loss    = outputs[\"loss\"]\n","\n","        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        return loss\n","    \n","    def training_epoch_end(self, outputs):\n","        loss = torch.tensor([o[\"loss\"] for o in outputs]).mean()\n","        \n","        self.log(\"train/epoch-loss\", loss)\n","        print(f\"#### Training {self.current_epoch} epoch mean loss: {loss} ####\")\n","        print(\"\\n\")\n","\n","\n","    def validation_step(self, batch, idx):\n","        outputs = self.model(**batch)\n","        outputs[\"labels\"] = batch[\"labels\"]\n","        return outputs\n","    \n","    def validation_epoch_end(self, outputs):\n","        logits = torch.cat([o[\"logits\"] for o in outputs]).detach().cpu().numpy()\n","        labels = torch.cat([o[\"labels\"] for o in outputs]).detach().cpu().numpy()\n","        loss   = torch.tensor([o[\"loss\"] for o in outputs]).mean()\n","        scores = compute_metrics((logits, labels))\n","\n","        self.log(\"val/loss\", loss)\n","        self.log_dict(scores)\n","        \n","        saved = False\n","        if scores[\"val/mean_score\"] < self.best_metric:\n","            self.best_metric = scores[\"val/mean_score\"]\n","            saved = True\n","        string = \"Saved\" if saved else \"Not saved\"\n","\n","        print(f\"#### Validation Loss: {loss} -- Mean Score: {scores['val/mean_score']} -- Model {string} ####\")\n","        \n","    def configure_optimizers(self):\n","        optimizer, scheduler = optimizers\n","        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n","\n","def train(model, optimizers, compute_metrics, eval_steps, train_dataloader, eval_dataloader, train=True):\n","        \n","        wrapped_model = AWPModule(model)\n","\n","        class DataHandler(pl.Callback):\n","            def on_train_epoch_end(self, trainer, pl_module):\n","                print(\"#### Incrementing current epoch count ####\")\n","                trainer.train_dataloader.loaders.dataset.epoch    += 1\n","                trainer.train_dataloader.loaders.collate_fn.epoch += 1\n","                                   \n","#         checkpoint = pl.callbacks.ModelCheckpoint(\n","#             dirpath           = CFG.save_dir,\n","#             monitor           = \"val/mean_score\",\n","#             mode              = \"min\",\n","#             save_weights_only = True,\n","#             save_last         = True,\n","#         )\n","\n","        trainer = pl.Trainer(\n","            logger                  = pl.loggers.WandbLogger(project=NAME_PROJECT) if CFG.wandb else True,\n","            callbacks               = [DataHandler(), pl.callbacks.LearningRateMonitor(\"step\")],\n","            accumulate_grad_batches = CFG.accumulation_steps,\n","            devices                 = 1,\n","            accelerator             = \"gpu\" if CFG.device == \"cuda\" else \"cpu\",\n","            gradient_clip_val       = CFG.max_grad_norm,\n","            log_every_n_steps       = 5,\n","            max_epochs              = CFG.max_epochs,\n","            precision               = 16 if CFG.fp16 else 32,\n","            enable_progress_bar     = True, \n","            val_check_interval      = eval_steps,\n","        )\n","\n","        if train:\n","            trainer.fit(wrapped_model, train_dataloader, eval_dataloader)\n","        else:\n","            trainer.validate(model=wrapped_model, dataloaders=eval_dataloader)\n","\n","### Other utilities ###\n","\n","def get_cols():\n","    return [IDS_TO_LABELS[i] for i in range(len(IDS_TO_LABELS))]\n","\n","def make_folds(df, num_folds, random_state=42):\n","    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","    \n","    df.loc[:, \"fold\"] = -1\n","    split_fn = MultilabelStratifiedKFold(n_splits=num_folds, shuffle=True, random_state=random_state)\n","    for n, (train, valid) in enumerate(split_fn.split(df, df[get_cols()])):\n","        df.loc[valid, \"fold\"] = n\n","    return df\n","\n","def clean_text(text):\n","    new_text = text.strip()\n","    return new_text\n","\n","def reinit_last_layers(transformer_model, num_layers):\n","    if num_layers > 0:\n","        transformer_model.encoder.layer[-num_layers:].apply(transformer_model._init_weights)"]},{"cell_type":"markdown","metadata":{"id":"wyjL1eTMK3QM"},"source":["### 4) Prepare Data"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T18:35:53.465646Z","iopub.status.busy":"2022-12-01T18:35:53.465345Z","iopub.status.idle":"2022-12-01T18:35:58.567606Z","shell.execute_reply":"2022-12-01T18:35:58.566664Z","shell.execute_reply.started":"2022-12-01T18:35:53.465611Z"},"id":"wvrR67V_j_ap","outputId":"7fea5dfd-5f6b-4aa9-850f-0cebf314aaab","trusted":true},"outputs":[],"source":["tokenizer = transformers.AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n","\n","if CFG.new_line_token:\n","    tokenizer.add_tokens([\"\\n\"], special_tokens=True)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T18:35:58.571757Z","iopub.status.busy":"2022-12-01T18:35:58.571494Z","iopub.status.idle":"2022-12-01T18:35:58.916557Z","shell.execute_reply":"2022-12-01T18:35:58.915559Z","shell.execute_reply.started":"2022-12-01T18:35:58.571723Z"},"id":"uCe_xGs6Ufyi","trusted":true},"outputs":[],"source":["nrows = 20 if CFG.debug else None\n","df = pd.read_csv(\"../input/feedback-prize-english-language-learning/train.csv\", nrows=nrows)\n","df = make_folds(df, CFG.num_folds)\n","df[\"full_text\"] = [clean_text(text) for text in df.full_text]"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T18:35:58.918141Z","iopub.status.busy":"2022-12-01T18:35:58.917883Z","iopub.status.idle":"2022-12-01T18:35:58.941211Z","shell.execute_reply":"2022-12-01T18:35:58.940117Z","shell.execute_reply.started":"2022-12-01T18:35:58.918109Z"},"id":"ijIC2iEzfU2A","outputId":"7bc7a795-102e-446c-d9d6-e67c9477b9f7","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_id</th>\n","      <th>full_text</th>\n","      <th>cohesion</th>\n","      <th>syntax</th>\n","      <th>vocabulary</th>\n","      <th>phraseology</th>\n","      <th>grammar</th>\n","      <th>conventions</th>\n","      <th>fold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0016926B079C</td>\n","      <td>I think that students would benefit from learn...</td>\n","      <td>3.5</td>\n","      <td>3.5</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0022683E9EA5</td>\n","      <td>When a problem is a change you have to let it ...</td>\n","      <td>2.5</td>\n","      <td>2.5</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.5</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00299B378633</td>\n","      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n","      <td>3.0</td>\n","      <td>3.5</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2.5</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>003885A45F42</td>\n","      <td>The best time in life is when you become yours...</td>\n","      <td>4.5</td>\n","      <td>4.5</td>\n","      <td>4.5</td>\n","      <td>4.5</td>\n","      <td>4.0</td>\n","      <td>5.0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0049B1DF5CCC</td>\n","      <td>Small act of kindness can impact in other peop...</td>\n","      <td>2.5</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2.5</td>\n","      <td>2.5</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        text_id                                          full_text  cohesion  \\\n","0  0016926B079C  I think that students would benefit from learn...       3.5   \n","1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n","2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n","3  003885A45F42  The best time in life is when you become yours...       4.5   \n","4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n","\n","   syntax  vocabulary  phraseology  grammar  conventions  fold  \n","0     3.5         3.0          3.0      4.0          3.0     1  \n","1     2.5         3.0          2.0      2.0          2.5     0  \n","2     3.5         3.0          3.0      3.0          2.5     4  \n","3     4.5         4.5          4.5      4.0          5.0     3  \n","4     3.0         3.0          3.0      2.5          2.5     1  "]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T18:35:58.943268Z","iopub.status.busy":"2022-12-01T18:35:58.942993Z","iopub.status.idle":"2022-12-01T18:35:58.962421Z","shell.execute_reply":"2022-12-01T18:35:58.961385Z","shell.execute_reply.started":"2022-12-01T18:35:58.943232Z"},"id":"Mqi01muhgdFE","trusted":true},"outputs":[],"source":["CFG.mask_prob_0 = 0\n","CFG.mix_prob_0 = 0\n","\n","\n","class CDataset(Dataset):\n","    def __init__(self, df, tokenizer, use_augmentations):\n","        self.df                = df\n","        self.tokenizer         = tokenizer\n","        self.use_augmentations = use_augmentations\n","        self.epoch             = 0\n","\n","        print(f\"{len(self)} Samples Loaded\")\n","\n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, idx):\n","        items   = self.df.iloc[idx]\n","        template = \"Evaluate the following text on: cohesion syntax vocabulary phraseology grammar conventions\"\n","        text = items[\"full_text\"]\n","#         encoded = self.tokenizer(template, text, truncation=True, max_length=50)\n","        encoded = self.tokenizer(template, text)\n","\n","        # Data Augmentation\n","        if self.use_augmentations:\n","            mask_prob = 0.0\n","            if self.epoch == 0: mask_prob = CFG.mask_prob_0\n","\n","            # Apply random mask \n","            if mask_prob > 0:\n","                ids            = torch.tensor(encoded[\"input_ids\"])\n","                probs          = torch.full(ids.shape, mask_prob)\n","                special_tokens = torch.tensor(self.tokenizer.get_special_tokens_mask(ids, already_has_special_tokens=True))\n","                probs.masked_fill_(special_tokens, 0.0)\n","\n","                masked_indices       = torch.bernoulli(probs).bool()\n","                ids[masked_indices]  = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n","                encoded[\"input_ids\"] = ids.tolist()\n","\n","        encoded[\"labels\"] = items[get_cols()].tolist()\n","        return encoded\n","\n","class CCollator:\n","    def __init__(self, tokenizer, use_augmentations):\n","        self.tokenizer         = tokenizer\n","        self.use_augmentations = use_augmentations\n","        self.epoch             = 0\n","        \n","    def __call__(self, batch):\n","        labels = torch.tensor([item.pop(\"labels\") for item in batch], dtype=torch.float32)\n","        batch  = self.tokenizer.pad(batch, return_tensors=\"pt\")\n","        # Data Augmentation\n","        if self.use_augmentations:\n","            mix_prob = 0.0\n","            if self.epoch == 0: mix_prob = CFG.mix_prob_0\n","            \n","            if mix_prob > 0:\n","                if random.random() < mix_prob:\n","                    ids   = batch[\"input_ids\"]\n","                    mask  = batch[\"attention_mask\"]\n","                    types = batch[\"token_type_ids\"]\n","                    \n","                    perm  = torch.randperm(ids.shape[0])\n","                    rand_len = int(ids.shape[1] * 0.25)\n","                    start = torch.randint(15, ids.shape[1] - rand_len, (1,)) # IMPORTANT: Hard coded min (len of template)\n","\n","                    ids[:, start:start+rand_len] = ids[perm, start:start+rand_len]\n","                    mask[:, start:start+rand_len] = mask[perm, start:start+rand_len]\n","                    types[:, start:start+rand_len] = types[perm, start:start+rand_len]\n","                    \n","                    \n","        batch[\"labels\"] = labels\n","        return batch\n","\n","def one_batch():\n","    ds       = CDataset(df, tokenizer, True)\n","    collator = CCollator(tokenizer, True)\n","    dl       = DataLoader(ds, batch_size=2, collate_fn=collator)\n","    return next(iter(dl))"]},{"cell_type":"markdown","metadata":{"id":"UaiAGI6WK-Zl"},"source":["### 5) Model, optimizer, loss and metrics"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T18:35:58.96488Z","iopub.status.busy":"2022-12-01T18:35:58.964322Z","iopub.status.idle":"2022-12-01T18:35:58.995148Z","shell.execute_reply":"2022-12-01T18:35:58.994265Z","shell.execute_reply.started":"2022-12-01T18:35:58.96476Z"},"id":"0Awgb31HUihC","trusted":true},"outputs":[],"source":["LOSS_FN = nn.MSELoss\n","# LOSS_FN = nn.SmoothL1Loss\n","# LOSS_FN = nn.L1Loss\n","\n","def compute_metrics(eval_preds):\n","    # Mean Columnwise Root Mean Squarred Error\n","    ids, labels = eval_preds\n","    scores      = []\n","    idxes       = labels.shape[1]\n","\n","    for i in range(idxes):\n","        pred  = ids[:, i]\n","        label = labels[:, i]\n","        score = mean_squared_error(label, pred, squared=False) # RMSE\n","        scores.append(score)\n","\n","    mean_score = np.mean(scores)\n","    metrics = {\"val/mean_score\": mean_score, **{f\"val/{v}\": scores[k] for k,v in IDS_TO_LABELS.items()}}\n","    return metrics\n","\n","\n","class MeanPooling(nn.Module):\n","    def __init__(self):\n","        super(MeanPooling, self).__init__()\n","        \n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        sum_mask = input_mask_expanded.sum(1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        mean_embeddings = sum_embeddings / sum_mask\n","        return mean_embeddings\n","\n","class AttentionPool(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(AttentionPool, self).__init__()\n","        self.attention = nn.Sequential(\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.GELU(),\n","            nn.Linear(hidden_size, 1),\n","        )\n","    \n","    def forward(self, hidden_state, attention_mask):\n","        w = self.attention(hidden_state)\n","        w[attention_mask == 0] = float(\"-inf\")\n","        w = torch.softmax(w, dim=1)\n","        context = torch.sum(hidden_state * w, dim=1)\n","        return context\n","\n","    \n","class LinearAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(LinearAttention, self).__init__()\n","        self.attention = nn.Linear(hidden_size, 1)\n","    \n","    def forward(self, hidden_state, attention_mask):\n","        w = self.attention(hidden_state)\n","        w[attention_mask == 0] = float(\"-inf\")\n","        w = torch.softmax(w, dim=1)\n","\n","        context = torch.sum(hidden_state * w, dim=1)\n","        return context\n","\n","\n","class MultiSampleHead(nn.Module):\n","    def __init__(self, hidden_size, num_labels):\n","        super(MultiSampleHead, self).__init__()\n","        self.drops   = nn.ModuleList([nn.Dropout(0.1 * i) for i in range(1,6)])\n","        self.cls     = nn.Linear(hidden_size, num_labels)\n","        self.loss_fn = LOSS_FN()\n","\n","    def forward(self, hidden_state, labels=None):\n","        loss   = 0\n","        logits = torch.stack([\n","            self.cls(drop(hidden_state)) for drop in self.drops\n","        ]).mean(dim=0)\n","\n","        if labels is not None:\n","            loss = self.loss_fn(logits, labels)\n","\n","        return {\n","            \"logits\" : logits,\n","            \"loss\"   : loss\n","        }\n","    \n","\n","class SimpleHead(nn.Module):\n","    def __init__(self, hidden_size, num_labels):\n","        super(SimpleHead, self).__init__()\n","        self.cls     = nn.Linear(hidden_size, num_labels)\n","        self.loss_fn = LOSS_FN()\n","    \n","    def forward(self, hidden_state, labels=None):\n","        loss         = 0\n","        hidden_state = self.cls(hidden_state)\n","        \n","        if labels is not None:\n","            loss = self.loss_fn(hidden_state, labels)\n","        \n","        return {\n","            \"logits\": hidden_state,\n","            \"loss\"  : loss\n","        }\n","\n","\n","class LSTMBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, num_layers=1, p_drop=0):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_size=in_channels,\n","                             hidden_size=out_channels,\n","                             num_layers=num_layers,\n","                             dropout=p_drop,\n","                             batch_first=True, \n","                             bidirectional=True)\n","    def forward(self, x):\n","        x,_ = self.lstm(x)\n","        return x\n","\n","class CNet(nn.Module):\n","    def __init__(self, model_name, num_labels, embed_size, pretrained=True):\n","        super(CNet, self).__init__()\n","        config = transformers.AutoConfig.from_pretrained(model_name)\n","        config.update({\n","            \"hidden_dropout_prob\"         : 0.0,\n","            \"attention_probs_dropout_prob\": 0.0,\n","            \"add_pooling_layer\"           : False,\n","            \"num_labels\"                  : num_labels,\n","            \"rnn_dropout\"                 : CFG.rnn_dropout,\n","            \"head_dropout\"                : CFG.head_dropout\n","        })\n","        \n","        self.config = config\n","        \n","        if pretrained:\n","            self.transformer = transformers.AutoModel.from_pretrained(model_name, config=config)\n","            self.transformer.resize_token_embeddings(embed_size)\n","        else:\n","            self.transformer = transformers.AutoModel.from_config(config)\n","        \n","        hidden_size = config.hidden_size * 4\n","        self.lstm = LSTMBlock(hidden_size, hidden_size // 2, p_drop=CFG.rnn_dropout)\n","\n","        self.pool = AttentionPool(hidden_size)\n","        \n","        self.head_drop = nn.Dropout(CFG.head_dropout)\n","        self.head = SimpleHead(hidden_size, num_labels)\n","        \n","        if CFG.gradient_checkpoint:\n","            self.transformer.gradient_checkpointing_enable()\n","        \n","        self._init_weights(self.head)\n","        \n","        reinit_last_layers(self.transformer, 1)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n","        out = self.transformer(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True).hidden_states\n","        \n","        out = torch.cat(out[-4:], dim=-1)\n","        \n","        out = self.lstm(out)\n","        \n","        out = self.pool(out, attention_mask)\n","\n","        out = self.head_drop(out)\n","\n","        out = self.head(out, labels)\n","        \n","        return out"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T18:35:58.996955Z","iopub.status.busy":"2022-12-01T18:35:58.996695Z","iopub.status.idle":"2022-12-01T18:35:59.015928Z","shell.execute_reply":"2022-12-01T18:35:59.015018Z","shell.execute_reply.started":"2022-12-01T18:35:58.996922Z"},"trusted":true},"outputs":[],"source":["def get_optimizer(CFG, model, total_steps, warm_steps):\n","\n","    all_parameters = list(model.named_parameters())\n","    used_name_parameters = set()\n","\n","    params = []\n","\n","    no_wd = [\"word_embeddings\", \"bias\", \"LayerNorm.weight\"]\n","\n","    # Head parameters\n","    head =  [(name, param) for name, param in all_parameters if not \"transformer\" in name]\n","    for n,_ in head: used_name_parameters.add(n)\n","    \n","    params.append({\"params\": [p for n,p in head if not any(nd in n for nd in no_wd)], \"weight_decay\": 0.01, \"lr\": CFG.head_lr})\n","    params.append({\"params\": [p for n,p in head if any(nd in n for nd in no_wd)], \"weight_decay\": 0., \"lr\": CFG.head_lr})\n","\n","    # Backend parameters\n","    transformer = [(name, param) for name, param in all_parameters if \"transformer\" in name]\n","    groups = [\n","        [ [\".embeddings.\"] ,                           1e-6],\n","        [ [\"encoder.LayerNorm\", \"rel_embeddings\"],     1e-6],\n","        [ [\".\" + str(i) + \".\" for i in range(0,6)],    1e-8],\n","        [ [\".\" + str(i) + \".\" for i in range(6,12)],   1e-7],\n","        [ [\".\" + str(i) + \".\" for i in range(12,23)],  CFG.transformer_lr],\n","        [ [\".23.\"] , CFG.head_lr ]\n","    ]\n","    \n","    for group in groups:\n","        grouped_names = group[0]\n","        lr = group[1]\n","\n","        parameters = [(name, param) for name,param in transformer if any(gn in name for gn in grouped_names)]\n","        for n, _ in parameters: used_name_parameters.add(n)\n","        \n","        params.append({\"params\": [p for n,p in parameters if not any(nd in n for nd in no_wd)], \"weight_decay\": 0.01, \"lr\": lr})\n","        params.append({\"params\": [p for n,p in parameters if any(nd in n for nd in no_wd)], \"weight_decay\": 0., \"lr\": lr})\n","\n","    state_dict_keys = {n:p for n,p in all_parameters}.keys()\n","    assert len(state_dict_keys - used_name_parameters) == 0,\\\n","    f\"Missing parameters: {str(state_dict_keys - used_name_parameters)}\"\n","    \n","    if not CFG.optimizer_bit8:\n","        optimizer = torch.optim.AdamW(params, eps=CFG.adam_eps)\n","    elif CFG.device == \"cuda\":\n","        optimizer = bnb.optim.AdamW8bit(params, eps=CFG.adam_eps)\n","    scheduler = transformers.get_scheduler(CFG.scheduler, optimizer, warm_steps, total_steps)\n","\n","    return (optimizer, scheduler)"]},{"cell_type":"markdown","metadata":{"id":"QO0oykF6LDws"},"source":["### 6) Train!!"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T18:35:59.017713Z","iopub.status.busy":"2022-12-01T18:35:59.017302Z","iopub.status.idle":"2022-12-01T18:35:59.029924Z","shell.execute_reply":"2022-12-01T18:35:59.028964Z","shell.execute_reply.started":"2022-12-01T18:35:59.017672Z"},"id":"1Idw2X-YTg3B","outputId":"d4e315e7-8142-43d4-b8a6-50352345180c","trusted":true},"outputs":[],"source":["# # Init wandb\n","CFG.wandb = False\n","if not CFG.debug and len(WANDB_SECRET) > 0:\n","    WANDB = wandb.login(key=WANDB_SECRET)\n","\n","    if WANDB:\n","        wandb.init(\n","            project=NAME_PROJECT,\n","            config={k:v for k,v in CFG.__dict__.items() if not k.startswith(\"_\")},\n","            save_code=True\n","        )\n","\n","    CFG.wandb = WANDB"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T18:35:59.031954Z","iopub.status.busy":"2022-12-01T18:35:59.031647Z","iopub.status.idle":"2022-12-01T18:35:59.047425Z","shell.execute_reply":"2022-12-01T18:35:59.046477Z","shell.execute_reply.started":"2022-12-01T18:35:59.031912Z"},"id":"mL-etsDhFNic","outputId":"0efe894d-4d74-4913-92c0-1c33a253a666","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["model_name: microsoft/deberta-v3-large\n","transformer_lr: 1e-06\n","head_lr: 0.0003\n","batch_size: 2\n","accumulation_steps: 1\n","warmup_epochs: 0.33333333\n","max_epochs: 5\n","adam_eps: 1e-05\n","scheduler: linear\n","max_grad_norm: 1\n","head_dropout: 0.2\n","rnn_dropout: 0.0\n","num_folds: 5\n","train_on_full_data: False\n","current_fold: 0\n","seed: 0\n","eval_batch_size: 8\n","debug: False\n","fp16: True\n","gradient_checkpoint: False\n","optimizer_bit8: False\n","use_augmentation: False\n","new_line_token: True\n","use_awp: True\n","adversarial_lr: 1e-05\n","adversarial_eps: 0.001\n","start_awp_epoch: 1\n","awp_steps: 1\n","num_jobs: 40\n","device: cuda\n","device_name: Tesla V100-SXM2-32GB\n","mask_prob_0: 0\n","mix_prob_0: 0\n","wandb: False\n"]}],"source":["for key, value in CFG.__dict__.items():\n","    if not key.startswith(\"_\"):\n","        print(f\"{key}: {value}\")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T18:35:59.049323Z","iopub.status.busy":"2022-12-01T18:35:59.048992Z","iopub.status.idle":"2022-12-01T18:36:49.458046Z","shell.execute_reply":"2022-12-01T18:36:49.457098Z","shell.execute_reply.started":"2022-12-01T18:35:59.049285Z"},"id":"CtSzp5Xxsic6","trusted":true},"outputs":[],"source":["model = CNet(CFG.model_name, len(IDS_TO_LABELS), len(tokenizer), True)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T18:36:49.460004Z","iopub.status.busy":"2022-12-01T18:36:49.459715Z","iopub.status.idle":"2022-12-01T18:36:49.488618Z","shell.execute_reply":"2022-12-01T18:36:49.487641Z","shell.execute_reply.started":"2022-12-01T18:36:49.45996Z"},"id":"E28woSWBpeIc","outputId":"2bd7e499-1346-4cce-c97b-571047f7d61a","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["3129 Samples Loaded\n","782 Samples Loaded\n","Using device: Tesla V100-SXM2-32GB\n","Training for a total of 7825 steps\n","Warming for 2608 steps\n","Total parameters to optimize 551417863\n","<class 'torch.nn.modules.loss.MSELoss'>\n"]}],"source":["train_df = df[df.fold != CFG.current_fold]\n","valid_df = df[df.fold == CFG.current_fold]\n","\n","valid_df.loc[:, \"lens\"] = [len(text) for text in valid_df.full_text]\n","valid_df = valid_df.sort_values(by=\"lens\", ascending=False)\n","\n","train_ds = CDataset(train_df, tokenizer, CFG.use_augmentation)\n","train_collator = CCollator(tokenizer, CFG.use_augmentation)\n","\n","valid_ds = CDataset(valid_df, tokenizer, False)\n","valid_collator = CCollator(tokenizer, False)\n","\n","train_dataloader = DataLoader(train_ds, CFG.batch_size, shuffle=True, collate_fn=train_collator, num_workers=CFG.num_jobs, pin_memory=True)\n","valid_dataloader = DataLoader(valid_ds, CFG.eval_batch_size, shuffle=False, collate_fn=valid_collator, num_workers=CFG.num_jobs, pin_memory=True)\n","\n","\n","total_steps = int(len(train_dataloader) / CFG.accumulation_steps * CFG.max_epochs)\n","\n","if isinstance(CFG.warmup_epochs, float):\n","    warm_steps = int(total_steps * CFG.warmup_epochs)\n","else:\n","    warm_steps = int(len(train_dataloader) * CFG.warmup_epochs)\n","\n","optimizers = get_optimizer(CFG, model, total_steps, warm_steps)\n","\n","total_parameters = sum([param.data.nelement() for param in model.parameters()])\n","\n","print(f\"Using device: {CFG.device_name}\")\n","print(f\"Training for a total of {total_steps} steps\")\n","print(f\"Warming for {warm_steps} steps\")\n","print(f\"Total parameters to optimize {total_parameters}\")\n","print(LOSS_FN)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T18:36:49.490412Z","iopub.status.busy":"2022-12-01T18:36:49.489904Z","iopub.status.idle":"2022-12-01T18:36:49.495472Z","shell.execute_reply":"2022-12-01T18:36:49.494544Z","shell.execute_reply.started":"2022-12-01T18:36:49.490375Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"737a060b71814fe78aa285a71aa9b8b0","version_major":2,"version_minor":0},"text/plain":["Validation sanity check: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["#### Validation Loss: 10.391693115234375 -- Mean Score: 3.217883348464966 -- Model Saved ####\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d86c448c2c7b4f1dac274e8268389d17","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["START AWP\n","AWP is training with f16 data: True\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f9755289608f4832af64f73d92fbf4e3","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["#### Validation Loss: 0.2596585750579834 -- Mean Score: 0.508123517036438 -- Model Saved ####\n","#### Training 0 epoch mean loss: 0.5906834602355957 ####\n","\n","\n","#### Incrementing current epoch count ####\n"]},{"ename":"AttributeError","evalue":"'functools.partial' object has no attribute 'epoch'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_30549/3498986207.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_30549/3462216357.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizers, compute_metrics, eval_steps, train_dataloader, eval_dataloader, train)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwrapped_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/envs/pku/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0mtrain_dataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         self._call_and_handle_interrupt(\n\u001b[0;32m--> 741\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m         )\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/envs/pku/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \"\"\"\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/envs/pku/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;31m# TODO: ckpt_path only in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/envs/pku/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/envs/pku/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1279\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/envs/pku/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/envs/pku/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/envs/pku/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_EVALUATE_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/envs/pku/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/envs/pku/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;31m# the global step is manually decreased here due to backwards compatibility with existing loggers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/envs/pku/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_run_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/envs/pku/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\u001b[0m in \u001b[0;36mon_run_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;31m# call train epoch end hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_train_epoch_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_epoch_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/envs/pku/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mcall_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m             \u001b[0mcallback_fx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_fx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m                 \u001b[0mcallback_fx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m             \u001b[0;31m# next call hook in lightningModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/envs/pku/lib/python3.7/site-packages/pytorch_lightning/trainer/callback_hook.py\u001b[0m in \u001b[0;36mon_train_epoch_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;34m\"\"\"Called when the epoch ends.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_validation_epoch_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_30549/3462216357.py\u001b[0m in \u001b[0;36mon_train_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#### Incrementing current epoch count ####\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m    \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;31m#         checkpoint = pl.callbacks.ModelCheckpoint(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'functools.partial' object has no attribute 'epoch'"]}],"source":["if CFG.device == \"cuda\":\n","    train(model, optimizers, compute_metrics, 1., train_dataloader, valid_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.9","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"vscode":{"interpreter":{"hash":"1925003cfa3979ae366740114cfe890bf8d7ad5b88e4afe0ec571fe261ed45e3"}}},"nbformat":4,"nbformat_minor":4}
